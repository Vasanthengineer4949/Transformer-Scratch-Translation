D_MODEL = 512 # Dimension of input token representation - embeddings
VOCAB_SIZE = 10000 # Number of tokens in vocabulary
MAX_SEQ_LEN = 128 # Maximum Sequence Length of Input and Output Sequence
NUM_STACKS = 6 # Number of encoder and decoder stacks
ATTN_DROPOUT = 0.2 # Quantify the dropout of how much in attention
EPS = 1e-6 # A small value to avoid zero division error while normalization
FF_DROPOUT = 0.2 # How much dropout in FFN
NUM_HEADS = 8 # Number of attention heads
RES_DROPOUT = 0.2 # Dropout for residual connection